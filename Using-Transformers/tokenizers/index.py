from transformers import BertTokenizer

'''
  Tokenizer是NLP管道的核心组件之一
  它们只有一个目的：将文本转换为可以由模型处理的喜数据。模型只能处理数字，因此分词器需要将我们的文本输入转换为数字数据。

  在NLP中，通常处理的数据都是原始文本
'''


# word-based 第一种类型的分词器

'''
  目标是将原始文本拆分为多个单词，并为每个单词找到一个数字表示形式：
'''

raw_inputs = 'Jim Henson was a nice puppeteer'

# 使用python的split方法将原始文本拆分为多个单词
tokenized_text = raw_inputs.split()

print(tokenized_text)

'''
  ['Jim', 'Henson', 'was', 'a', 'nice', 'puppeteer']
'''

'''
  还有一些变体的单词分词器具有额外的标点符号规则。使用这种分词器，我们最终可以得到一些相当大的 “词汇表”，其中词汇表由语料库中独立词元的总数定义。
  每个单词都分配有一个ID，从0开始，一直到词汇表的大小。该哦模型使用这些ID来识别每个单词


  如果我们想用基于单词的分词器完全覆盖一种语言，我们需要为语言中的每个单词都有一个标识符，这将生成大量的分词。
  例如，英语中有超过 500,000 个单词，因此要构建从每个单词到输入 ID 的映射，我们需要跟踪这么多 ID。
  此外，像 “dog” 这样的词与 “dogs” 这样的词的表示方式不同，模型最初无法知道 “dog” 和 “dogs” 是相似的：它会将这两个词识别为不相关。

  最后，我们需要一个自定义标记来表示不在词汇表中的单词。这称之为"unknown" token。“【UNK】”

  如果你看到分词器正在产生大量这样的标记，这通常是一个坏兆头，因为它无法检索单词的合理表示，并且在此过程中你会丢失信息。
  制作词汇表时的目标是以这样一种方式进行，即分词器将尽可能少的单词分词到未知词元中。
'''

# Character-based 第二种类型的分词器 基于字符

'''
  基于字符的分词器将文本拆分为字符，而不是单词，主要有两个好处：
  - 词汇量要小得多
  - 词汇外 "[UNK]" 标记的数量要少得多，因为每个单词都可以由字符构建

  但是，这种方法也并不完美
  由于现在的表示是基于字符而不是单词，因此可以争辩说，
  从直觉上讲，它没有那么有意义：每个字符本身并没有太多含义，而单词就是这种情况。但是，这又因语言而异;例如，在中文中，每个字符比拉丁语中的字符承载更多的信息。

  由于现在的表示是基于字符而不是单词，因此可以争辩说，从直觉上讲，它没有那么有意义：每个字符本身并没有太多含义，而单词就是这种情况。但是，这又因语言而异;例如，在中文中，每个字符比拉丁语中的字符承载更多的信息。

  两全其美的选择是：子词分词化(subword tokenization)
'''

# Subword-based 第三种分词器

'''
  子词分词化(subword tokenization)是基于字符和单词分词器之间的折中方案。
  它将单词拆分为子词单元，而不是字符，但这些子词单元仍然是单词的一部分。
  例如，单词 “annoyingly” 可以拆分为 “annoy” 和 “ingly”。
  这允许我们捕获单词的结构，同时保持词汇量较小。

  原则是：经常使用的词不应该拆分为较小的词，而应将稀有词分解为有意义的子词
'''

# 加载和保存分词器简单

'''
  - from_pretrained() 方法
  - save_pretrained() 方法
'''

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

print(tokenizer)

# 保存分词器
tokenizer.save_pretrained("bert-base-uncased")

# 加载分词器
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

encode_string = tokenizer.encode("Hello, I'm a language model")

'''
  将文本转换为数字称之为编码。
  编码分为两个步骤：
  1. 分词化：将文本拆分为单词，tokens。有多个规则可以控制这个过程，这就是我们为什么需要使用模型的名称实例化分词器，以确保我们使用与模型训练时使用相同的规则
  2. 输入ID：将单词转换为数字。可以从中构建一个张量，并将其传递给模型。分词器有一个词汇表，这是我们使用from_pretrained() 方法加载分词器时自动下载的。
'''